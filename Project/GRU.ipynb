{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer # type: ignore\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences # type: ignore\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, Dropout\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "from tensorflow.keras.layers import GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Saves.HelperFunctions import *\n",
    "from Preprocessing import preprocess_text, category_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bool = True # set to True to include the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('Data/train.csv')\n",
    "if test_bool:\n",
    "    test_data = pd.read_csv('Data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SampleID</th>\n",
       "      <th>Discussion</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Without sitting down and doing it manually, yo...</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>All your Search ends with this link.</td>\n",
       "      <td>STEM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SampleID                                         Discussion Category\n",
       "0         1  Without sitting down and doing it manually, yo...   Sports\n",
       "1         2               All your Search ends with this link.     STEM"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SampleID                                         Discussion\n",
      "0         1  Managing cash flow effectively is crucial for ...\n",
      "1         2  Civic engagement plays a key role in a democra...\n",
      "2         3  Proper warm-ups and cool-downs are essential t...\n"
     ]
    }
   ],
   "source": [
    "if test_bool:\n",
    "    print(test_data.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop Nan...\n",
      "\ttrain_data.shape before (24989, 3)\n",
      "\ttrain_data.shape after (24646, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Drop Nan...')\n",
    "print(f\"\\ttrain_data.shape before {train_data.shape}\")\n",
    "train_data = train_data.dropna(subset=['Discussion'])\n",
    "print(f\"\\ttrain_data.shape after {train_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_method = 2\n",
    "\n",
    "# NLP preprocessing on text\n",
    "train_Discussion_preprocessed = [preprocess_text(discussion, pre_method) for discussion in train_data['Discussion']]\n",
    "if test_bool:\n",
    "    test_Discussion_preprocessed = [preprocess_text(discussion, pre_method) for discussion in test_data['Discussion']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction\n",
    "\n",
    "num_words = 20000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words)  # Set max vocabulary size\n",
    "tokenizer.fit_on_texts(train_Discussion_preprocessed) # Fit tokenizer on training data \n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(train_Discussion_preprocessed)\n",
    "if test_bool:    \n",
    "    X_test_seq = tokenizer.texts_to_sequences(test_Discussion_preprocessed)\n",
    "\n",
    "seq_len = 100\n",
    "\n",
    "# Padding\n",
    "X_train = pad_sequences(X_train_seq, maxlen=seq_len, padding='post')\n",
    "if test_bool:    \n",
    "    X_test = pad_sequences(X_test_seq, maxlen=seq_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Y_train\n",
    "Y_train = train_data['Category'].map(category_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (24646, 100)\n",
      "X_test.shape: (10557, 100)\n",
      "Y_train.shape: (24646,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train.shape: {X_train.shape}\")\n",
    "if test_bool:    \n",
    "    print(f\"X_test.shape: {X_test.shape}\")\n",
    "print(f\"Y_train.shape: {Y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    4\n",
       "2    4\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_words: 20000\n",
      "seq_len: 100\n"
     ]
    }
   ],
   "source": [
    "print(f\"num_words: {num_words}\")\n",
    "print(f\"seq_len: {seq_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\mawad\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\mawad\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\layers\\normalization\\layer_normalization.py:328: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\mawad\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 128)          2560000   \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 100, 512)          592896    \n",
      " al)                                                             \n",
      "                                                                 \n",
      " layer_normalization (Layer  (None, 100, 512)          1024      \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100, 512)          0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 100, 256)          493056    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " layer_normalization_1 (Lay  (None, 100, 256)          512       \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 100, 256)          0         \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirecti  (None, 128)               123648    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               66048     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 1285      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3969797 (15.14 MB)\n",
      "Trainable params: 3969797 (15.14 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout, LayerNormalization\n",
    "\n",
    "num_classes = 5\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding layer to convert word indices into dense vectors of fixed size\n",
    "model.add(Embedding(input_dim=num_words, output_dim=128, input_length=seq_len, trainable=True,\n",
    "        embeddings_regularizer=l2(0.0005)))\n",
    "model.add(Bidirectional(GRU(256, return_sequences=True, kernel_regularizer=l2(0.0005))))\n",
    "model.add(LayerNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Bidirectional(GRU(128, return_sequences=True, kernel_regularizer=l2(0.0005))))\n",
    "model.add(LayerNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Bidirectional(GRU(64, return_sequences=False, kernel_regularizer=l2(0.0005))))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(512, activation='relu', kernel_regularizer=l2(0.0005)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Dense(256, activation='relu', kernel_regularizer=l2(0.0005)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 9\n",
    "batch_size = 32\n",
    "validation_split = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming Y_train contains labels in integer form (e.g., [0, 1, 2, 3, 4])\n",
    "Y_train_categorical = to_categorical(Y_train, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/9\n",
      "WARNING:tensorflow:From c:\\Users\\mawad\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\mawad\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "617/617 [==============================] - 284s 448ms/step - loss: 2.1124 - accuracy: 0.3755 - val_loss: 1.6491 - val_accuracy: 0.5533\n",
      "Epoch 2/9\n",
      "617/617 [==============================] - 268s 434ms/step - loss: 1.5105 - accuracy: 0.5957 - val_loss: 1.4308 - val_accuracy: 0.6207\n",
      "Epoch 3/9\n",
      "617/617 [==============================] - 262s 425ms/step - loss: 1.2953 - accuracy: 0.6577 - val_loss: 1.3241 - val_accuracy: 0.6262\n",
      "Epoch 4/9\n",
      "617/617 [==============================] - 304s 492ms/step - loss: 1.1762 - accuracy: 0.6967 - val_loss: 1.2593 - val_accuracy: 0.6564\n",
      "Epoch 5/9\n",
      "617/617 [==============================] - 328s 532ms/step - loss: 1.0637 - accuracy: 0.7296 - val_loss: 1.2733 - val_accuracy: 0.6588\n",
      "Epoch 6/9\n",
      "617/617 [==============================] - 325s 527ms/step - loss: 1.0028 - accuracy: 0.7532 - val_loss: 1.2748 - val_accuracy: 0.6544\n",
      "Epoch 7/9\n",
      "617/617 [==============================] - 322s 522ms/step - loss: 0.9372 - accuracy: 0.7736 - val_loss: 1.3111 - val_accuracy: 0.6359\n",
      "Epoch 8/9\n",
      "617/617 [==============================] - 282s 456ms/step - loss: 0.8980 - accuracy: 0.7919 - val_loss: 1.4400 - val_accuracy: 0.6412\n",
      "Epoch 9/9\n",
      "617/617 [==============================] - 280s 454ms/step - loss: 0.8430 - accuracy: 0.8082 - val_loss: 1.3390 - val_accuracy: 0.6365\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x18587082850>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train_categorical, epochs=epochs, batch_size=batch_size, validation_split=validation_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "771/771 [==============================] - 75s 96ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predict the labels for the test set\n",
    "train_predictions = model.predict(X_train)\n",
    "\n",
    "# If it's a multi-class classification task, get the predicted class for each sample\n",
    "Y_train_pred = np.argmax(train_predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8052422299764668\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = accuracy_score(Y_train_pred, Y_train)\n",
    "print(f\"Train Accuracy: {train_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU-m2-e9-a80 saved successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mawad\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "if input('Press 0 to save the model') == '0':\n",
    "    file_name = f'GRU-m{pre_method}-e{epochs}-a{int(train_accuracy*100)}'\n",
    "    model.save(f'Models/RNN/{file_name}.h5')\n",
    "    print(f'{file_name} saved successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330/330 [==============================] - 27s 82ms/step\n",
      "Data saved to Saves/GRU-m2-e9-predictions.csv\n"
     ]
    }
   ],
   "source": [
    "if test_bool:    \n",
    "    # Predict the labels for the test set\n",
    "    test_predictions = model.predict(X_test)\n",
    "\n",
    "    # If it's a multi-class classification task, get the predicted class for each sample\n",
    "    Y_test_pred = np.argmax(test_predictions, axis=1)\n",
    "\n",
    "    if input('Press 0 to save the test predictions') == '0':\n",
    "        file_name = f'GRU-m{pre_method}-e{epochs}-predictions'\n",
    "        save_csv(data=Y_test_pred, file_name=file_name, header=['SampleID', 'Category'], numbering=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
