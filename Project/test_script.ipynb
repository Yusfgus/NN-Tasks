{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import load_model\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tkinter import filedialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Saves.HelperFunctions import *\n",
    "from Preprocessing import preprocess_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['Discussion'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Managing cash flow effectively is crucial for ...\n",
       "1    Civic engagement plays a key role in a democra...\n",
       "Name: Discussion, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "saves_dir = 'Delivaries'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Funcions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ffnn_preprocess(test_data, pre_method):\n",
    "\n",
    "    print('start preprocessing...')\n",
    "    test_Discussion_preprocessed = [preprocess_text(discussion, pre_method) for discussion in test_data['Discussion']]\n",
    "    print('TF-IDF...')\n",
    "    with open(f'Saves/{saves_dir}/tfidf_vectorizer.pkl', 'rb') as file:\n",
    "        vectorizer = pickle.load(file)\n",
    "        print('vectorizer loaded successfully...')\n",
    "\n",
    "    X_test = vectorizer.transform(test_Discussion_preprocessed)\n",
    "\n",
    "    return X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffnn_X_test = ffnn_preprocess(test_data=test_data, pre_method=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ffnn_model_path = 'FFNN-m4-e2-a92.h5'\n",
    "ffnn_model_path = filedialog.askopenfilename(filetypes=[(\"h5 Files\", \"*.h5\")])\n",
    "ffnn_model = load_model(ffnn_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffnn_predictions = ffnn_model.predict(ffnn_X_test)\n",
    "\n",
    "# If it's a multi-class classification task, get the predicted class for each sample\n",
    "ffnn_Y_pred = np.argmax(ffnn_predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input('Press 0 to save the predictions') == '0':\n",
    "    save_csv(data=ffnn_Y_pred, file_name=f'{saves_dir}/FFNN', header=['SampleID', 'Category'], numbering=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru_preprocess(X, pre_method):\n",
    "\n",
    "    print('start preprocessing...')\n",
    "    test_Discussion_preprocessed = [preprocess_text(discussion, pre_method) for discussion in X]\n",
    "    with open(f'Saves/{saves_dir}/tokenizer.pkl', 'rb') as file:\n",
    "        tokenizer = pickle.load(file)\n",
    "        print('tokenizer loaded successfully...')\n",
    "\n",
    "    X_test_seq = tokenizer.texts_to_sequences(test_Discussion_preprocessed)\n",
    "\n",
    "    seq_length = 100\n",
    "    X_test_padded = pad_sequences(X_test_seq, maxlen=seq_length, padding='post')\n",
    "\n",
    "    return X_test_padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start preprocessing...\n"
     ]
    }
   ],
   "source": [
    "gru_X_test_padded = gru_preprocess(X=X, pre_method=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# gru_model_path = 'Models\\\\RNN\\\\GRU-e10-a81.h5'\n",
    "# gru_model_path = 'Models\\\\RNN\\\\GRU-m2-e9-a85'\n",
    "gru_model_path = file_path = filedialog.askopenfilename(title=\"Select a file\", filetypes=[(\"h5 files\", \"*.h5\")])\n",
    "gru_model = load_model(gru_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predict the labels for the test set\n",
    "gru_predictions = gru_model.predict(gru_X_test_padded)\n",
    "\n",
    "# If it's a multi-class classification task, get the predicted class for each sample\n",
    "gru_Y_pred = np.argmax(gru_predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to Saves/Delivaries/GRU.csv\n"
     ]
    }
   ],
   "source": [
    "if input('Press 0 to save the predictions') == '0':\n",
    "    save_csv(data=gru_Y_pred, file_name=f'{saves_dir}/GRU', header=['SampleID', 'Category'], numbering=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
